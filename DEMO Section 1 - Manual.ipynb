{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "005a231c",
   "metadata": {},
   "source": [
    "Basic Python 101 : \n",
    "- Variables - int, float, string\n",
    "- Variable assignments\n",
    "- Library import and call"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c30c92",
   "metadata": {},
   "source": [
    "### 1. Tokenization - tokenization by space\n",
    "Tokenization is breaking the raw text into small chunks. Tokenization breaks the raw text into words, sentences called tokens. These tokens help in understanding the context or developing the model for the NLP. The tokenization helps in interpreting the meaning of the text by analyzing the sequence of the words.\n",
    "For example, the text “It is raining” can be tokenized into ‘It’, ‘is’, ‘raining’\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2594fe65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla is looking at buying U.S startup for $6 million. Tesla best seller product is at Tesla Model S.\n"
     ]
    }
   ],
   "source": [
    "doc1 = \"Tesla is looking at buying U.S startup for $6 million. Tesla best seller product is at Tesla Model S.\"\n",
    "print(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "69c677c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tesla',\n",
       " 'is',\n",
       " 'looking',\n",
       " 'at',\n",
       " 'buying',\n",
       " 'U.S.',\n",
       " 'startup',\n",
       " 'for',\n",
       " '$6',\n",
       " 'million.',\n",
       " 'Tesla',\n",
       " 'best',\n",
       " 'seller',\n",
       " 'product',\n",
       " 'is',\n",
       " 'Tesla',\n",
       " 'Model',\n",
       " 'S.']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1e05c12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 19,\t Number of unique tokens: 15\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of tokens: {len(doc1.split(' '))},\\t Number of unique tokens: {len(set(doc1.split(' ')))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "77202b82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$6',\n",
       " 'Model',\n",
       " 'S.',\n",
       " 'Tesla',\n",
       " 'U.S.',\n",
       " 'at',\n",
       " 'best',\n",
       " 'buying',\n",
       " 'for',\n",
       " 'is',\n",
       " 'looking',\n",
       " 'million.',\n",
       " 'product',\n",
       " 'seller',\n",
       " 'startup'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(doc1.split(\" \"))\n",
    "# doc1 = \"Tesla is looking at buying U.S. startup for $6 million. Tesla best seller product is at Tesla Model S.\" #remove one sentence one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "588e3acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tesla',\n",
       " 'is',\n",
       " 'looking',\n",
       " 'at',\n",
       " 'buying',\n",
       " 'U.S.',\n",
       " 'startup',\n",
       " 'for',\n",
       " '$6',\n",
       " 'million.',\n",
       " 'Tesla',\n",
       " 'best',\n",
       " 'seller',\n",
       " 'product',\n",
       " 'is',\n",
       " 'Tesla',\n",
       " 'Model',\n",
       " 'S.']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1.split(\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dde450",
   "metadata": {},
   "source": [
    "### 2. Tokenization - Tokenization by n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a455d809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual sentence:  Tesla is looking at buying U.S. startup for $6 million\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Tesla is',\n",
       " 'is looking',\n",
       " 'looking at',\n",
       " 'at buying',\n",
       " 'buying U.S.',\n",
       " 'U.S. startup',\n",
       " 'startup for',\n",
       " 'for $6',\n",
       " '$6 million']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_N_grams(text,ngram=1):\n",
    "    words = [word for word in text.split(\" \") ]\n",
    "    temp=zip(*[words[i:] for i in range(0,ngram)])\n",
    "    ans=[' '.join(ngram) for ngram in temp]\n",
    "    return ans\n",
    "\n",
    "print(\"Actual sentence: \", doc1)\n",
    "generate_N_grams(doc1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "62b6f854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Tesla', 'is', 'looking', 'at', 'buying')\n",
      "('is', 'looking', 'at', 'buying', 'U.S')\n",
      "('looking', 'at', 'buying', 'U.S', 'startup')\n",
      "('at', 'buying', 'U.S', 'startup', 'for')\n",
      "('buying', 'U.S', 'startup', 'for', '$6')\n",
      "('U.S', 'startup', 'for', '$6', 'million.')\n",
      "('startup', 'for', '$6', 'million.', 'Tesla')\n",
      "('for', '$6', 'million.', 'Tesla', 'best')\n",
      "('$6', 'million.', 'Tesla', 'best', 'seller')\n",
      "('million.', 'Tesla', 'best', 'seller', 'product')\n",
      "('Tesla', 'best', 'seller', 'product', 'is')\n",
      "('best', 'seller', 'product', 'is', 'at')\n",
      "('seller', 'product', 'is', 'at', 'Tesla')\n",
      "('product', 'is', 'at', 'Tesla', 'Model')\n",
      "('is', 'at', 'Tesla', 'Model', 'S.')\n"
     ]
    }
   ],
   "source": [
    "ngrams = 5\n",
    "words = doc1.split(\" \")\n",
    "temp = []\n",
    "for i in range(0,ngrams):\n",
    "    temp.append(words[i:])\n",
    "temp = zip(*temp)\n",
    "\n",
    "for ngram in temp:\n",
    "    print(ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "8fc1f588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Dr', 'Rohana')\n",
      "('Rohana', 'has')\n",
      "('has', 'invite')\n",
      "('invite', 'me')\n",
      "('me', 'to')\n",
      "('to', 'attend')\n",
      "('attend', 'this')\n",
      "('this', 'class')\n",
      "('class', 'today.')\n"
     ]
    }
   ],
   "source": [
    "def n_grams_tokenization(ngrams, doc):\n",
    "#     ngrams = 5\n",
    "    words = doc.split(\" \")\n",
    "    temp = []\n",
    "    for i in range(0,ngrams):\n",
    "        temp.append(words[i:])\n",
    "    temp = zip(*temp)\n",
    "\n",
    "    for ngram in temp:\n",
    "        print(ngram)\n",
    "        \n",
    "n_grams_tokenization(ngrams = 2, doc = 'Dr Rohana has invite me to attend this class today.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd674097",
   "metadata": {},
   "source": [
    "### 3. Stemming - Regex\n",
    "Stemming is a technique used to extract the base form of the words by removing affixes from them. It is just like cutting down the branches of a tree to its stems. For example, the stem of the words eating, eats, eaten is eat.\n",
    "\n",
    "Search engines use stemming for indexing the words. That’s why rather than storing all forms of a word, a search engine can store only the stems. In this way, stemming reduces the size of the index and increases retrieval accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7a64631d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = 'Dr Rohana likes to eat nasi lemak for dinner while Dr Norisma and Amin enjoying their foods .'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c3625de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dr\n",
      "Rohana\n",
      "like\n",
      "to\n",
      "eat\n",
      "nasi\n",
      "lemak\n",
      "for\n",
      "dinner\n",
      "while\n",
      "Dr\n",
      "Norisma\n",
      "and\n",
      "Amin\n",
      "enjoy\n",
      "their\n",
      "food\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for word in doc2.split(\" \"):\n",
    "    if word.endswith('s'):\n",
    "        print(word[:-1])\n",
    "    elif word.endswith('ing'):\n",
    "        print(word[:-3])\n",
    "    else:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590202bd",
   "metadata": {},
   "source": [
    "### 3. Stemming - Porter Stemmer intuitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e008e70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PorterStemmer:\n",
    "    def isCons(self, letter):\n",
    "        if letter == 'a' or letter == 'e' or letter == 'i' or letter == 'o' \\\n",
    "        or letter == 'u':\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "\n",
    "    def isConsonant(self, word, i):\n",
    "        letter = word[i]\n",
    "        if self.isCons(letter):\n",
    "            if letter == 'y' and isCons(word[i-1]):\n",
    "                return False\n",
    "            else:\n",
    "                return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def isVowel(self, word, i):\n",
    "        return not(self.isConsonant(word, i))\n",
    "\n",
    "    # *S\n",
    "    def endsWith(self, stem, letter):\n",
    "        if stem.endswith(letter):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    # *v*\n",
    "    def containsVowel(self, stem):\n",
    "        for i in stem:\n",
    "            if not self.isCons(i):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    # *d\n",
    "    def doubleCons(self, stem):\n",
    "        if len(stem) >= 2:\n",
    "            if self.isConsonant(stem, -1) and self.isConsonant(stem, -2):\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def getForm(self, word):\n",
    "        form = []\n",
    "        formStr = ''\n",
    "        for i in range(len(word)):\n",
    "            if self.isConsonant(word, i):\n",
    "                if i != 0:\n",
    "                    prev = form[-1]\n",
    "                    if prev != 'C':\n",
    "                        form.append('C')\n",
    "                else:\n",
    "                    form.append('C')\n",
    "            else:\n",
    "                if i != 0:\n",
    "                    prev = form[-1]\n",
    "                    if prev != 'V':\n",
    "                        form.append('V')\n",
    "                else:\n",
    "                    form.append('V')\n",
    "        for j in form:\n",
    "            formStr += j\n",
    "        return formStr\n",
    "\n",
    "    def getM(self, word):\n",
    "        form = self.getForm(word)\n",
    "        m = form.count('VC')\n",
    "        return m\n",
    "\n",
    "    # *o\n",
    "    def cvc(self, word):\n",
    "        if len(word) >= 3:\n",
    "            f = -3\n",
    "            s = -2\n",
    "            t = -1\n",
    "            third = word[t]\n",
    "            if self.isConsonant(word, f) and self.isVowel(word, s) \\\n",
    "            and self.isConsonant(word, t):\n",
    "                if third != 'w' and third != 'x' and third != 'y':\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "            else:\n",
    "                return False\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def replace(self, orig, rem, rep):\n",
    "        result = orig.rfind(rem)\n",
    "        base = orig[:result]\n",
    "        replaced = base + rep\n",
    "        return replaced\n",
    "\n",
    "    def replaceM0(self, orig, rem, rep):\n",
    "        result = orig.rfind(rem)\n",
    "        base = orig[:result]\n",
    "        if self.getM(base) > 0:\n",
    "            replaced = base + rep\n",
    "            return replaced\n",
    "        else:\n",
    "            return orig\n",
    "\n",
    "    def replaceM1(self, orig, rem, rep):\n",
    "        result = orig.rfind(rem)\n",
    "        base = orig[:result]\n",
    "        if self.getM(base) > 1:\n",
    "            replaced = base + rep\n",
    "            return replaced\n",
    "        else:\n",
    "            return orig\n",
    "\n",
    "    def step1a(self, word):\n",
    "        if word.endswith('sses'):\n",
    "            word = self.replace(word, 'sses', 'ss')\n",
    "        elif word.endswith('ies'):\n",
    "            word = self.replace(word, 'ies', 'i')\n",
    "        elif word.endswith('ss'):\n",
    "            word = self.replace(word, 'ss', 'ss')\n",
    "        elif word.endswith('s'):\n",
    "            word = self.replace(word, 's', '')\n",
    "        else:\n",
    "            pass\n",
    "        return word\n",
    "\n",
    "    def step1b(self, word):\n",
    "        flag = False\n",
    "        if word.endswith('eed'):\n",
    "            result = word.rfind('eed')\n",
    "            base = word[:result]\n",
    "            if self.getM(base) > 0:\n",
    "                word = base\n",
    "                word += 'ee'\n",
    "        elif word.endswith('ed'):\n",
    "            result = word.rfind('ed')\n",
    "            base = word[:result]\n",
    "            if self.containsVowel(base):\n",
    "                word = base\n",
    "                flag = True\n",
    "        elif word.endswith('ing'):\n",
    "            result = word.rfind('ing')\n",
    "            base = word[:result]\n",
    "            if self.containsVowel(base):\n",
    "                word = base\n",
    "                flag = True\n",
    "        if flag:\n",
    "            if word.endswith('at') or word.endswith('bl') or word.endswith('iz'):\n",
    "                word += 'e'\n",
    "            elif self.doubleCons(word) and not self.endsWith(word, 'l') \\\n",
    "            and not self.endsWith(word, 's') and not self.endsWith(word, 'z'):\n",
    "                word = word[:-1]\n",
    "            elif self.getM(word) == 1 and self.cvc(word):\n",
    "                word += 'e'\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "        return word\n",
    "\n",
    "    def step1c(self, word):\n",
    "        if word.endswith('y'):\n",
    "            result = word.rfind('y')\n",
    "            base = word[:result]\n",
    "            if self.containsVowel(base):\n",
    "                word = base\n",
    "                word += 'i'\n",
    "        return word\n",
    "\n",
    "    def step2(self, word):\n",
    "        if word.endswith('ational'):\n",
    "            word = self.replaceM0(word, 'ational', 'ate')\n",
    "        elif word.endswith('tional'):\n",
    "            word = self.replaceM0(word, 'tional', 'tion')\n",
    "        elif word.endswith('enci'):\n",
    "            word = self.replaceM0(word, 'enci', 'ence')\n",
    "        elif word.endswith('anci'):\n",
    "            word = self.replaceM0(word, 'anci', 'ance')\n",
    "        elif word.endswith('izer'):\n",
    "            word = self.replaceM0(word, 'izer', 'ize')\n",
    "        elif word.endswith('abli'):\n",
    "            word = self.replaceM0(word, 'abli', 'able')\n",
    "        elif word.endswith('alli'):\n",
    "            word = self.replaceM0(word, 'alli', 'al')\n",
    "        elif word.endswith('entli'):\n",
    "            word = self.replaceM0(word, 'entli', 'ent')\n",
    "        elif word.endswith('eli'):\n",
    "            word = self.replaceM0(word, 'eli', 'e')\n",
    "        elif word.endswith('ousli'):\n",
    "            word = self.replaceM0(word, 'ousli', 'ous')\n",
    "        elif word.endswith('ization'):\n",
    "            word = self.replaceM0(word, 'ization', 'ize')\n",
    "        elif word.endswith('ation'):\n",
    "            word = self.replaceM0(word, 'ation', 'ate')\n",
    "        elif word.endswith('ator'):\n",
    "            word = self.replaceM0(word, 'ator', 'ate')\n",
    "        elif word.endswith('alism'):\n",
    "            word = self.replaceM0(word, 'alism', 'al')\n",
    "        elif word.endswith('iveness'):\n",
    "            word = self.replaceM0(word, 'iveness', 'ive')\n",
    "        elif word.endswith('fulness'):\n",
    "            word = self.replaceM0(word, 'fulness', 'ful')\n",
    "        elif word.endswith('ousness'):\n",
    "            word = self.replaceM0(word, 'ousness', 'ous')\n",
    "        elif word.endswith('aliti'):\n",
    "            word = self.replaceM0(word, 'aliti', 'al')\n",
    "        elif word.endswith('iviti'):\n",
    "            word = self.replaceM0(word, 'iviti', 'ive')\n",
    "        elif word.endswith('biliti'):\n",
    "            word = self.replaceM0(word, 'biliti', 'ble')\n",
    "        return word\n",
    "\n",
    "    def step3(self, word):\n",
    "        if word.endswith('icate'):\n",
    "            word = self.replaceM0(word, 'icate', 'ic')\n",
    "        elif word.endswith('ative'):\n",
    "            word = self.replaceM0(word, 'ative', '')\n",
    "        elif word.endswith('alize'):\n",
    "            word = self.replaceM0(word, 'alize', 'al')\n",
    "        elif word.endswith('iciti'):\n",
    "            word = self.replaceM0(word, 'iciti', 'ic')\n",
    "        elif word.endswith('ful'):\n",
    "            word = self.replaceM0(word, 'ful', '')\n",
    "        elif word.endswith('ness'):\n",
    "            word = self.replaceM0(word, 'ness', '')\n",
    "        return word\n",
    "\n",
    "    def step4(self, word):\n",
    "        if word.endswith('al'):\n",
    "            word = self.replaceM1(word, 'al', '')\n",
    "        elif word.endswith('ance'):\n",
    "            word = self.replaceM1(word, 'ance', '')\n",
    "        elif word.endswith('ence'):\n",
    "            word = self.replaceM1(word, 'ence', '')\n",
    "        elif word.endswith('er'):\n",
    "            word = self.replaceM1(word, 'er', '')\n",
    "        elif word.endswith('ic'):\n",
    "            word = self.replaceM1(word, 'ic', '')\n",
    "        elif word.endswith('able'):\n",
    "            word = self.replaceM1(word, 'able', '')\n",
    "        elif word.endswith('ible'):\n",
    "            word = self.replaceM1(word, 'ible', '')\n",
    "        elif word.endswith('ant'):\n",
    "            word = self.replaceM1(word, 'ant', '')\n",
    "        elif word.endswith('ement'):\n",
    "            word = self.replaceM1(word, 'ement', '')\n",
    "        elif word.endswith('ment'):\n",
    "            word = self.replaceM1(word, 'ment', '')\n",
    "        elif word.endswith('ent'):\n",
    "            word = self.replaceM1(word, 'ent', '')\n",
    "        elif word.endswith('ou'):\n",
    "            word = self.replaceM1(word, 'ou', '')\n",
    "        elif word.endswith('ism'):\n",
    "            word = self.replaceM1(word, 'ism', '')\n",
    "        elif word.endswith('ate'):\n",
    "            word = self.replaceM1(word, 'ate', '')\n",
    "        elif word.endswith('iti'):\n",
    "            word = self.replaceM1(word, 'iti', '')\n",
    "        elif word.endswith('ous'):\n",
    "            word = self.replaceM1(word, 'ous', '')\n",
    "        elif word.endswith('ive'):\n",
    "            word = self.replaceM1(word, 'ive', '')\n",
    "        elif word.endswith('ize'):\n",
    "            word = self.replaceM1(word, 'ize', '')\n",
    "        elif word.endswith('ion'):\n",
    "            result = word.rfind('ion')\n",
    "            base = word[:result]\n",
    "            if self.getM(base) > 1 \\\n",
    "            and (self.endsWith(base, 's') or self.endsWith(base, 't')):\n",
    "                word = base\n",
    "            word = self.replaceM1(word, '', '')\n",
    "        return word\n",
    "\n",
    "    def step5a(self, word):\n",
    "        if word.endswith('e'):\n",
    "            base = word[:-1]\n",
    "            if self.getM(base) > 1:\n",
    "                word = base\n",
    "            elif self.getM(base) == 1 and not self.cvc(base):\n",
    "                word = base\n",
    "        return word\n",
    "\n",
    "    def step5b(self, word):\n",
    "        if self.getM(word) > 1 and self.doubleCons(word) \\\n",
    "        and self.endsWith(word, 'l'):\n",
    "            word = word[:-1]\n",
    "        return word\n",
    "\n",
    "    def stem(self, word):\n",
    "        word = self.step1a(word)\n",
    "        word = self.step1b(word)\n",
    "        word = self.step1c(word)\n",
    "        word = self.step2(word)\n",
    "        word = self.step3(word)\n",
    "        word = self.step4(word)\n",
    "        word = self.step5a(word)\n",
    "        word = self.step5b(word)\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9d061d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = PorterStemmer()\n",
    "ps.stem(\"eats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80e711c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a7f6f04",
   "metadata": {},
   "source": [
    "### 4. Lemmatization\n",
    "\n",
    "Lemmatization technique is like stemming. The output we will get after lemmatization is called ‘lemma’, which is a root word rather than root stem, the output of stemming. After lemmatization, we will be getting a valid word that means the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "43f13ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "5d55d29b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\aminh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "89a321ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "08602baf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eating'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('eating')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b191acdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'book'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('books')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ec390c",
   "metadata": {},
   "source": [
    "### Difference between Stemming & Lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ca55327e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'believ'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "word_stemmer = PorterStemmer()\n",
    "word_stemmer.stem('believes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8fa80710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'belief'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatizer.lemmatize('believes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e8efdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdfa88a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd0ae23d",
   "metadata": {},
   "source": [
    "### 5. Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2e6851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8e2f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
